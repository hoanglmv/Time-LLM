# -*- coding: utf-8 -*-
# File n√†y d√πng ƒë·ªÉ ch·∫°y th·ª≠ nghi·ªám m·ªôt v√≤ng hu·∫•n luy·ªán (training loop) nhanh
# v·ªõi m√¥ h√¨nh DLinear ƒë·ªÉ ki·ªÉm tra l·ªói v√† ƒë·∫£m b·∫£o c√°c th√†nh ph·∫ßn c·ªët l√µi ho·∫°t ƒë·ªông ƒë√∫ng.
# N√≥ s·ª≠ d·ª•ng m·ªôt c·∫•u h√¨nh t·ªëi gi·∫£n, kh√¥ng d√πng command-line arguments.

import torch
from torch import nn, optim
from tqdm import tqdm
import time
import random
import numpy as np
import os
import sys

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc c·ªßa d·ª± √°n v√†o sys.path
# ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c√°c module nh∆∞ 'models', 'prepare_data' c√≥ th·ªÉ ƒë∆∞·ª£c import
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from models import DLinear
from prepare_data.data_provider.data_factory import data_provider

# --- C·∫•u h√¨nh cho vi·ªác ch·∫°y nhanh ---
class FastConfig:
    def __init__(self):
        # Basic
        self.is_training = 1
        self.model_id = 'DLinear_fast_test'
        self.model = 'DLinear'
        self.task_name = 'long_term_forecast'
        
        # Data
        self.data = 'ETTh1'
        self.root_path = './dataset/ETT-small/'
        self.data_path = 'ETTh1.csv'
        self.features = 'M' # Multivariate
        self.target = 'OT'
        self.freq = 'h'
        self.checkpoints = './checkpoints/'
        self.percent = 10 # Ch·ªâ d√πng 10% d·ªØ li·ªáu

        # Model & Task
        self.seq_len = 96
        self.label_len = 48
        self.pred_len = 24
        self.enc_in = 7
        self.dec_in = 7
        self.c_out = 7
        self.individual = False # DLinear specific
        self.moving_avg = 25

        # Training
        self.train_epochs = 1
        self.batch_size = 16
        self.learning_rate = 0.001
        self.loss = 'MSE'
        self.num_workers = 0 # 0 ƒë·ªÉ debug d·ªÖ h∆°n tr√™n Windows
        self.embed = 'timeF' # Kh√¥ng qu√° quan tr·ªçng v·ªõi DLinear nh∆∞ng c·∫ßn cho data_provider
        self.seasonal_patterns= 'Monthly' # Placeholder

def run_fast_test():
    """
    H√†m ch√≠nh ƒë·ªÉ ch·∫°y th·ª≠ nghi·ªám nhanh.
    """
    args = FastConfig()
    
    # Thi·∫øt l·∫≠p device (GPU n·∫øu c√≥, kh√¥ng th√¨ CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {device}")

    # --- T·∫£i d·ªØ li·ªáu ---
    print("\n[1/4] ‚è≥ ƒêang t·∫£i d·ªØ li·ªáu...")
    try:
        train_data, train_loader = data_provider(args, 'train')
        print(f"‚úÖ T·∫£i d·ªØ li·ªáu th√†nh c√¥ng. S·ªë l∆∞·ª£ng batch training: {len(train_loader)}")
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {e}")
        print("üí° G·ª£i √Ω: B·∫°n ƒë√£ t·∫£i v√† ƒë·∫∑t dataset v√†o ƒë√∫ng th∆∞ m·ª•c './dataset/ETT-small/' ch∆∞a?")
        return

    # --- Kh·ªüi t·∫°o m√¥ h√¨nh ---
    print("\n[2/4] ‚öôÔ∏è  ƒêang kh·ªüi t·∫°o m√¥ h√¨nh DLinear...")
    model = DLinear.Model(args).float().to(device)
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    criterion = nn.MSELoss()
    print("‚úÖ Kh·ªüi t·∫°o m√¥ h√¨nh th√†nh c√¥ng.")

    # --- V√≤ng l·∫∑p Hu·∫•n luy·ªán ---
    print(f"\n[3/4] üî• B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán nhanh cho {args.train_epochs} epoch...")
    model.train()
    epoch_time = time.time()

    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(train_loader), total=len(train_loader)):
        optimizer.zero_grad()

        # ƒê∆∞a d·ªØ li·ªáu l√™n device
        batch_x = batch_x.float().to(device)
        batch_y = batch_y.float().to(device)
        batch_x_mark = batch_x_mark.float().to(device)
        batch_y_mark = batch_y_mark.float().to(device)

        # T·∫°o decoder input
        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()
        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)

        # Forward pass
        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)

        # L·∫•y ph·∫ßn output t∆∞∆°ng ·ª©ng v·ªõi pred_len
        f_dim = -1 if args.features == 'MS' else 0
        outputs = outputs[:, -args.pred_len:, f_dim:]
        batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)
        
        loss = criterion(outputs, batch_y)
        
        # Backward pass v√† c·∫≠p nh·∫≠t tr·ªçng s·ªë
        loss.backward()
        optimizer.step()

        if i % 50 == 0: # In loss ƒë·ªãnh k·ª≥
            print(f"\n   Batch {i}/{len(train_loader)} | Loss: {loss.item():.6f}")

    print(f"‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t trong {time.time() - epoch_time:.2f} gi√¢y.")

    # --- Ho√†n t·∫•t ---
    print("\n[4/4] üéâ Ch·∫°y th·ª≠ nghi·ªám th√†nh c√¥ng!")
    print("   - M√¥ h√¨nh DLinear ƒë√£ ch·∫°y qua 1 epoch m√† kh√¥ng c√≥ l·ªói runtime.")
    print("   - Pipeline d·ªØ li·ªáu v√† training ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng.")

if __name__ == '__main__':
    # Thi·∫øt l·∫≠p seed ƒë·ªÉ k·∫øt qu·∫£ c√≥ th·ªÉ t√°i l·∫≠p
    fix_seed = 2021
    random.seed(fix_seed)
    torch.manual_seed(fix_seed)
    np.random.seed(fix_seed)

    run_fast_test()
