# -*- coding: utf-8 -*-
# File n√†y cho ph√©p b·∫°n t√πy ch·ªânh c√°c tham s·ªë v√† ch·∫°y th·ª≠ nghi·ªám m·ªôt c√°ch d·ªÖ d√†ng.
# Ch·ªâ c·∫ßn ch·ªânh s·ª≠a c√°c gi√° tr·ªã trong l·ªõp `Config` d∆∞·ªõi ƒë√¢y v√† ch·∫°y file.

import torch
from torch import nn, optim
from tqdm import tqdm
import time
import random
import numpy as np
import os
import sys

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc c·ªßa d·ª± √°n v√†o sys.path ƒë·ªÉ import c√°c module t√πy ch·ªânh
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from models import Autoformer, DLinear, TimeLLM
from prepare_data.data_provider.data_factory import data_provider
from utils.tools import EarlyStopping, adjust_learning_rate, vali, load_content
from utils.visualize import plot_loss, plot_classification_metrics

# =================================================================================
# =================== KHU V·ª∞C C·∫§U H√åNH (CONFIGURATION AREA) ===================
# =================================================================================
# Ch·ªânh s·ª≠a c√°c gi√° tr·ªã trong l·ªõp `Config` n√†y ƒë·ªÉ ch·∫°y th·ª≠ nghi·ªám c·ªßa b·∫°n.
class Config:
    def __init__(self):
        # --- C·∫•u h√¨nh Th·ª≠ nghi·ªám Ch√≠nh (Thay ƒë·ªïi ·ªü ƒë√¢y) ---
        self.model = 'DLinear'              # Model: 'DLinear', 'Autoformer', 'TimeLLM'
        self.data = 'ETTh1'                 # Dataset: 'ETTh1', 'ETTh2', 'ETTm1', 'ETTm2', 'Weather', 'ECL', 'Traffic'
        self.train_epochs = 3               # S·ªë epochs ƒë·ªÉ hu·∫•n luy·ªán
        self.learning_rate = 0.005          # T·ªëc ƒë·ªô h·ªçc (learning rate)
        self.batch_size = 32                # K√≠ch th∆∞·ªõc batch

        # --- C·∫•u h√¨nh T√°c v·ª• & M√¥ h√¨nh (Thay ƒë·ªïi n·∫øu c·∫ßn) ---
        self.is_training = 1                # 1: Hu·∫•n luy·ªán; 0: Ch·ªâ test
        self.task_name = 'long_term_forecast'
        self.seq_len = 96                   # ƒê·ªô d√†i chu·ªói ƒë·∫ßu v√†o
        self.pred_len = 96                  # ƒê·ªô d√†i chu·ªói d·ª± ƒëo√°n
        self.label_len = 48                 # ƒê·ªô d√†i c·ªßa start token
        
        # --- C·∫•u h√¨nh D·ªØ li·ªáu (Th∆∞·ªùng kh√¥ng c·∫ßn ƒë·ªïi n·∫øu d√πng dataset chu·∫©n) ---
        self.root_path = './dataset/ETT-small/' # ƒê∆∞·ªùng d·∫´n g·ªëc t·ªõi th∆∞ m·ª•c dataset
        self.data_path = 'ETTh1.csv'        # T√™n file d·ªØ li·ªáu
        self.features = 'M'                 # 'M', 'S', 'MS'
        self.target = 'OT'                  # C·ªôt m·ª•c ti√™u
        self.freq = 'h'                     # T·∫ßn su·∫•t: 'h' (gi·ªù), 't' (ph√∫t), 'd' (ng√†y)
        self.percent = 100                  # Ph·∫ßn trƒÉm d·ªØ li·ªáu s·ª≠ d·ª•ng (10-100)

        # --- C·∫•u h√¨nh Chi ti·∫øt M√¥ h√¨nh (N√¢ng cao) ---
        # T·ª± ƒë·ªông ƒëi·ªÅn m·ªôt s·ªë tham s·ªë d·ª±a tr√™n dataset
        self.enc_in, self.dec_in, self.c_out = self.get_dims_based_on_data()
        
        self.d_model = 512                  # K√≠ch th∆∞·ªõc embedding c·ªßa model
        self.d_ff = 2048                    # K√≠ch th∆∞·ªõc l·ªõp Feed-Forward
        self.n_heads = 8                    # S·ªë l∆∞·ª£ng attention heads
        self.e_layers = 2                   # S·ªë l·ªõp encoder
        self.d_layers = 1                   # S·ªë l·ªõp decoder
        self.dropout = 0.1
        self.moving_avg = 25                # K√≠ch th∆∞·ªõc c·ª≠a s·ªï cho DLinear v√† Autoformer
        self.individual = False             # D√†nh ri√™ng cho DLinear
        self.llm_layers = 6                 # S·ªë l·ªõp LLM cho TimeLLM

        # --- C·∫•u h√¨nh L∆∞u tr·ªØ & T·ªëi ∆∞u h√≥a ---
        self.model_id = f'{self.model}_{self.data}_sl{self.seq_len}_pl{self.pred_len}'
        self.checkpoints = './checkpoints/'
        self.loss = 'MSE'
        self.lradj = 'type1'
        self.patience = 3
        self.num_workers = 0 # 0 ƒë·ªÉ debug d·ªÖ h∆°n tr√™n Windows
        self.embed = 'timeF'
        self.seasonal_patterns= 'Monthly' # Placeholder cho dataset M4

    def get_dims_based_on_data(self):
        """T·ª± ƒë·ªông tr·∫£ v·ªÅ s·ªë features cho c√°c b·ªô d·ªØ li·ªáu ti√™u chu·∫©n."""
        data_dims = {
            'ETTh1': (7, 7, 7),
            'ETTh2': (7, 7, 7),
            'ETTm1': (7, 7, 7),
            'ETTm2': (7, 7, 7),
            'Weather': (21, 21, 21),
            'ECL': (321, 321, 321),
            'Traffic': (862, 862, 862),
        }
        return data_dims.get(self.data, (1, 1, 1)) # M·∫∑c ƒë·ªãnh l√† 1 n·∫øu kh√¥ng t√¨m th·∫•y


# =================================================================================
# =================== M√É TH·ª∞C THI (EXECUTION CODE) ===================
# =================================================================================
# B·∫°n th∆∞·ªùng kh√¥ng c·∫ßn ch·ªânh s·ª≠a ph·∫ßn code b√™n d∆∞·ªõi.

def run_custom():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y th·ª≠ nghi·ªám v·ªõi c·∫•u h√¨nh ƒë√£ ƒë·ªãnh nghƒ©a."""
    args = Config()
    
    # Thi·∫øt l·∫≠p device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {device}")
    
    # Thi·∫øt l·∫≠p seed ƒë·ªÉ k·∫øt qu·∫£ c√≥ th·ªÉ t√°i l·∫≠p
    fix_seed = 2021
    random.seed(fix_seed)
    torch.manual_seed(fix_seed)
    np.random.seed(fix_seed)

    # --- T·∫£i d·ªØ li·ªáu ---
    print("\n[1/4] ‚è≥ ƒêang t·∫£i d·ªØ li·ªáu...")
    try:
        train_data, train_loader = data_provider(args, 'train')
        vali_data, vali_loader = data_provider(args, 'val')
        test_data, test_loader = data_provider(args, 'test')
        print(f"‚úÖ T·∫£i d·ªØ li·ªáu '{args.data}' th√†nh c√¥ng. {len(train_loader)} training batches.")
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {e}")
        print(f"üí° G·ª£i √Ω: H√£y ch·∫Øc ch·∫Øn r·∫±ng 'root_path' v√† 'data_path' trong Config l√† ch√≠nh x√°c.")
        return

    # --- Kh·ªüi t·∫°o m√¥ h√¨nh ---
    print(f"\n[2/4] ‚öôÔ∏è  ƒêang kh·ªüi t·∫°o m√¥ h√¨nh {args.model}...")
    if args.model == 'Autoformer':
        model = Autoformer.Model(args).float().to(device)
    elif args.model == 'DLinear':
        model = DLinear.Model(args).float().to(device)
    else: # TimeLLM
        model = TimeLLM.Model(args).float().to(device)
    
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    criterion = nn.MSELoss() if args.task_name != 'classification' else nn.CrossEntropyLoss()
    early_stopping = EarlyStopping(patience=args.patience, verbose=True)
    
    save_path = os.path.join(args.checkpoints, args.model_id)
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    
    print(f"‚úÖ Kh·ªüi t·∫°o m√¥ h√¨nh th√†nh c√¥ng. Checkpoint s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {save_path}")

    # --- Hu·∫•n luy·ªán ---
    if args.is_training:
        print(f"\n[3/4] üî• B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán cho {args.train_epochs} epochs...")
        
        # L·ªãch s·ª≠ ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
        history = {
            'train_loss': [],
            'val_loss': [],
            'test_loss': [],
            'acc': [],
            'f1': [],
            'precision': [],
            'recall': []
        }

        for epoch in range(args.train_epochs):
            model.train()
            epoch_time = time.time()
            train_loss = []
            
            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f"Epoch {epoch+1}/{args.train_epochs}"):
                optimizer.zero_grad()

                batch_x = batch_x.float().to(device)
                batch_y = batch_y.float().to(device)
                batch_x_mark = batch_x_mark.float().to(device)
                batch_y_mark = batch_y_mark.float().to(device)

                dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()
                dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)
                
                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                
                if args.task_name == 'classification':
                    loss = criterion(outputs, batch_y.long().squeeze())
                else:
                    f_dim = -1 if args.features == 'MS' else 0
                    outputs = outputs[:, -args.pred_len:, f_dim:]
                    batch_y = batch_y[:, -args.pred_len:, f_dim:]
                    loss = criterion(outputs, batch_y)
                
                train_loss.append(loss.item())
                loss.backward()
                optimizer.step()

            avg_train_loss = np.average(train_loss)
            history['train_loss'].append(avg_train_loss)
            print(f"\nEpoch {epoch + 1} | Time: {time.time() - epoch_time:.2f}s | Train Loss: {avg_train_loss:.6f}")
            
            # ƒê√°nh gi√° tr√™n b·ªô validation v√† test
            vali_results = vali(args, None, model, vali_data, vali_loader, criterion, nn.L1Loss())
            test_results = vali(args, None, model, test_data, test_loader, criterion, nn.L1Loss())
            
            history['val_loss'].append(vali_results['loss'])
            history['test_loss'].append(test_results['loss'])

            if args.task_name == 'classification':
                print(f"              Vali Loss: {vali_results['loss']:.4f} | Acc: {vali_results['acc']:.4f} | F1: {vali_results['f1']:.4f}")
                history['acc'].append(vali_results['acc'])
                history['f1'].append(vali_results['f1'])
                history['precision'].append(vali_results['precision'])
                history['recall'].append(vali_results['recall'])
            else:
                print(f"              Vali Loss: {vali_results['loss']:.6f} | Test Loss: {test_results['loss']:.6f}")

            early_stopping(vali_results['loss'], model, save_path)
            if early_stopping.early_stop:
                print("Early stopping!")
                break
            
            adjust_learning_rate(None, optimizer, None, epoch + 1, args, printout=True)
            
        print("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t.")
        
        # --- V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì ---
        print("\n[---] üìà ƒêang t·∫°o v√† l∆∞u bi·ªÉu ƒë·ªì...")
        fig_save_path = os.path.join('figures', args.model_id)
        plot_loss(history['train_loss'], history['val_loss'], save_path=f"{fig_save_path}_loss.png")
        if args.task_name == 'classification':
            plot_classification_metrics({k: v for k, v in history.items() if k in ['acc', 'f1', 'precision', 'recall']},
                                        save_path=f"{fig_save_path}_metrics.png")

    # --- ƒê√°nh gi√° cu·ªëi c√πng ---
    print("\n[4/4] üìä ƒêang t·∫£i model t·ªët nh·∫•t v√† th·ª±c hi·ªán ƒë√°nh gi√° cu·ªëi c√πng...")
    best_model_path = os.path.join(save_path, 'checkpoint.pth')
    if os.path.exists(best_model_path):
        model.load_state_dict(torch.load(best_model_path))
        print("‚úÖ T·∫£i model t·ªët nh·∫•t th√†nh c√¥ng.")
        final_results = vali(args, None, model, test_data, test_loader, criterion, nn.L1Loss())
        
        if args.task_name == 'classification':
            print(f"üéâ Ho√†n t·∫•t! K·∫øt qu·∫£ cu·ªëi c√πng tr√™n b·ªô test -> Loss: {final_results['loss']:.4f} | Acc: {final_results['acc']:.4f} | F1: {final_results['f1']:.4f}")
        else:
            print(f"üéâ Ho√†n t·∫•t! K·∫øt qu·∫£ cu·ªëi c√πng tr√™n b·ªô test -> Loss (MSE): {final_results['loss']:.6f} | MAE: {final_results['mae']:.6f}")
    else:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y checkpoint t·∫°i {best_model_path}. Kh√¥ng th·ªÉ th·ª±c hi·ªán ƒë√°nh gi√° cu·ªëi c√πng.")

if __name__ == '__main__':
    run_custom()
